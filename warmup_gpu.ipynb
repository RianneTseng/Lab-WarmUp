{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1ajU52dFyUyCs84xInmRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d93fdd92e98d46bd90f006a7354c9c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14673c3b6c6d479f8adb2f4cecc45aeb",
              "IPY_MODEL_dbbe535eefba4f4f9ca4d2c75aea9b75",
              "IPY_MODEL_4afcb255e6414b9197074fbb15ca96e5"
            ],
            "layout": "IPY_MODEL_2c485c1d12af4099b547d84a3925bcd3"
          }
        },
        "14673c3b6c6d479f8adb2f4cecc45aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f940839316634636acb419951124a24c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d1c0114c4304494817aadfaeca83760",
            "value": "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf: 100%"
          }
        },
        "dbbe535eefba4f4f9ca4d2c75aea9b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614e601219964e7fba6f94318d6a062c",
            "max": 667822976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e5b1e507403479e9641fdb35038c61c",
            "value": 667822976
          }
        },
        "4afcb255e6414b9197074fbb15ca96e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80bf01cb104840e680e73c16b9767ac1",
            "placeholder": "​",
            "style": "IPY_MODEL_e8cd72f8941c4ccfa30c54baee68c0f3",
            "value": " 668M/668M [00:03&lt;00:00, 222MB/s]"
          }
        },
        "2c485c1d12af4099b547d84a3925bcd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f940839316634636acb419951124a24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d1c0114c4304494817aadfaeca83760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "614e601219964e7fba6f94318d6a062c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5b1e507403479e9641fdb35038c61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80bf01cb104840e680e73c16b9767ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8cd72f8941c4ccfa30c54baee68c0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d895ea98f66e45df868895d3c2ff8552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be035c18730c4b6b83ade9285e7e2250",
              "IPY_MODEL_4c588c55335a4f818dc225686df0ac03",
              "IPY_MODEL_647445d09c1a423e9301af0bab351beb"
            ],
            "layout": "IPY_MODEL_85ccaee92048447a84841fd549bd9947"
          }
        },
        "be035c18730c4b6b83ade9285e7e2250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102bc15118e44365927e2b919c164a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_b36cee81e683402a95be27d060cc3665",
            "value": "Fetching 10 files: 100%"
          }
        },
        "4c588c55335a4f818dc225686df0ac03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7157ddd184154c308045a8b82d4b42c8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66a4cdd4988449d79f1ec6053ab353e4",
            "value": 10
          }
        },
        "647445d09c1a423e9301af0bab351beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863b30347ca943e8a6022e23a8ef3aac",
            "placeholder": "​",
            "style": "IPY_MODEL_932908a370684392a398f9955338ede3",
            "value": " 10/10 [00:00&lt;00:00,  8.15it/s]"
          }
        },
        "85ccaee92048447a84841fd549bd9947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102bc15118e44365927e2b919c164a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36cee81e683402a95be27d060cc3665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7157ddd184154c308045a8b82d4b42c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a4cdd4988449d79f1ec6053ab353e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "863b30347ca943e8a6022e23a8ef3aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932908a370684392a398f9955338ede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f429f430583a475e81f6030d4c98c959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1946c169e68f4d91b7c248b27ac80009",
              "IPY_MODEL_481f027208344e879dcffbd0981c6a4e",
              "IPY_MODEL_c733a65d9ede4ff6b9ed40dc07d29cb1"
            ],
            "layout": "IPY_MODEL_ea21f5cb61284d478d1c5f490bb50bc0"
          }
        },
        "1946c169e68f4d91b7c248b27ac80009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3261e6e59674b15a516e431663d7522",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf0f70b6db1407c887daf02d6017e89",
            "value": "tokenizer.json: 100%"
          }
        },
        "481f027208344e879dcffbd0981c6a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a34d30ab2443ed9feb325701ade596",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_021a45ed513f40e5a30fd386c5973e9c",
            "value": 1842767
          }
        },
        "c733a65d9ede4ff6b9ed40dc07d29cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d197f0dd5bf40449b8328986e4df0a3",
            "placeholder": "​",
            "style": "IPY_MODEL_77702880934e4ab4a4518631cde16aa5",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 4.45MB/s]"
          }
        },
        "ea21f5cb61284d478d1c5f490bb50bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3261e6e59674b15a516e431663d7522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf0f70b6db1407c887daf02d6017e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85a34d30ab2443ed9feb325701ade596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021a45ed513f40e5a30fd386c5973e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d197f0dd5bf40449b8328986e4df0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77702880934e4ab4a4518631cde16aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpjssr/Lab-WarmUp/blob/main/warmup_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
        "#!sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "#!sudo apt-get update\n",
        "#!sudo apt-get -y install cuda-toolkit-12-5"
      ],
      "metadata": {
        "id": "_MIe4F4dBQ6m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8I5QyeSCAJLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31dfd88a-c387-45ba-fd22-59ccb418ba1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=294849 sha256=52a48c1e01af67c15779cf2c234449a16e6add9ea3d738170a1eac898faea386\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import snapshot_download\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ZOM6V_stP9rc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"/content/drive/MyDrive/lab/gpu\"\n",
        "llama_folder = root_folder + \"/llama.cpp\"\n",
        "hf_folder = root_folder + \"/hf\"\n",
        "output_folder = root_folder + \"/output\"\n",
        "\n",
        "main_path = llama_folder + \"/main\"\n",
        "quantize_path = llama_folder + \"/quantize\"\n",
        "\n",
        "hf_token=\"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\"\n",
        "gguf_model_name = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\"\n",
        "hf_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_1_id = \"tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\"\n",
        "model_2_hf = root_folder + \"/hf\"\n",
        "model_2_id = \"TinyLlama/\" + hf_model_name\n",
        "model_2_path = root_folder + \"/\" + hf_model_name + \"-f16.gguf\"\n",
        "model_3_path = root_folder + \"/\" + hf_model_name + \"-Q8.gguf\"\n",
        "model_1_output_path = output_folder + \"/model_1.txt\"\n",
        "model_2_output_path = output_folder + \"/model_2.txt\"\n",
        "model_3_output_path = output_folder + \"/model_3.txt\"\n",
        "\n",
        "problem_set=[\n",
        "\"Explain the concept of gravity.\",\n",
        "\"Continue this conversation: So, do you think it will rain tomorrow?\",\n",
        "\"What is the derivative of x^2+3x+2?\",\n",
        "\"Discuss the significance of the Lunar New Year in East Asian cultures.\",\n",
        "\"Describe how a blockchain works.\",\n",
        "\"Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\",\n",
        "\"Write a short story about a detective solving a mystery in a futuristic city.\",\n",
        "\"Translate 'Hello, how are you?' into Japanese.\",\n",
        "\"Tell a joke about artificial intelligence.\",\n",
        "\"What are the implications of the latest financial news from Wall Street?\"\n",
        "]"
      ],
      "metadata": {
        "id": "GlKPcJhwP--g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd {llama_folder}\n",
        "!chmod 755 {main_path}\n",
        "!chmod 755 {quantize_path}"
      ],
      "metadata": {
        "id": "CYxCWZaqBIDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef28b344-2a7c-4bb2-9725-34e2661539a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/lab/gpu/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Challenge"
      ],
      "metadata": {
        "id": "6co6DtiAUpDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1_id)\n",
        "print(root_folder)\n",
        "print(gguf_model_name)\n",
        "model_1_path = hf_hub_download(\n",
        "               filename=model_1_id,\n",
        "               local_dir=root_folder,\n",
        "               token=hf_token,\n",
        "               repo_id=gguf_model_name)"
      ],
      "metadata": {
        "id": "O5v3L_dwAxrq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "d93fdd92e98d46bd90f006a7354c9c1a",
            "14673c3b6c6d479f8adb2f4cecc45aeb",
            "dbbe535eefba4f4f9ca4d2c75aea9b75",
            "4afcb255e6414b9197074fbb15ca96e5",
            "2c485c1d12af4099b547d84a3925bcd3",
            "f940839316634636acb419951124a24c",
            "7d1c0114c4304494817aadfaeca83760",
            "614e601219964e7fba6f94318d6a062c",
            "9e5b1e507403479e9641fdb35038c61c",
            "80bf01cb104840e680e73c16b9767ac1",
            "e8cd72f8941c4ccfa30c54baee68c0f3"
          ]
        },
        "outputId": "45d5d8e7-50c7-41bd-98de-b0f5c39387d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\n",
            "/content/drive/MyDrive/lab/gpu\n",
            "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf:   0%|          | 0.00/668M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d93fdd92e98d46bd90f006a7354c9c1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_1_path} --n-predict 100 --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "A647sdgeBL5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9be1385-cdff-42e5-a354-558724821299"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560650\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_K:  135 tensors\n",
            "llama_model_loader: - type q6_K:   21 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 262/32003 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32003\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   375.36 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   117.78 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s>ints of a deafening noise. The screaming stopped. \"Oh, what? What is that?\"\n",
            "Jin-Rohan's voice was barely a few seconds later.\n",
            "\"What is that?\"\n",
            "\"I told you, what is that!\"\n",
            "Jin-chan was a screamed as he didn't even heard as he could barely hear a loud, \"What noise?\"\n",
            "He looked at the sound of the noise. \"Shit\n",
            "llama_print_timings:        load time =    2075.22 ms\n",
            "llama_print_timings:      sample time =       5.33 ms /   100 runs   (    0.05 ms per token, 18775.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    5172.27 ms /   100 runs   (   51.72 ms per token,    19.33 tokens per second)\n",
            "llama_print_timings:       total time =    5249.58 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Medium Challenge"
      ],
      "metadata": {
        "id": "VyOv0Z97TcEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_download(\n",
        "    repo_id=model_2_id,\n",
        "    local_dir=model_2_hf,\n",
        "    local_dir_use_symlinks=False,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=hf_token)\n",
        "!python convert.py {hf_folder} --outfile {model_2_path} --outtype f16"
      ],
      "metadata": {
        "id": "LYDmuMZzA1eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d895ea98f66e45df868895d3c2ff8552",
            "be035c18730c4b6b83ade9285e7e2250",
            "4c588c55335a4f818dc225686df0ac03",
            "647445d09c1a423e9301af0bab351beb",
            "85ccaee92048447a84841fd549bd9947",
            "102bc15118e44365927e2b919c164a3b",
            "b36cee81e683402a95be27d060cc3665",
            "7157ddd184154c308045a8b82d4b42c8",
            "66a4cdd4988449d79f1ec6053ab353e4",
            "863b30347ca943e8a6022e23a8ef3aac",
            "932908a370684392a398f9955338ede3",
            "f429f430583a475e81f6030d4c98c959",
            "1946c169e68f4d91b7c248b27ac80009",
            "481f027208344e879dcffbd0981c6a4e",
            "c733a65d9ede4ff6b9ed40dc07d29cb1",
            "ea21f5cb61284d478d1c5f490bb50bc0",
            "c3261e6e59674b15a516e431663d7522",
            "dcf0f70b6db1407c887daf02d6017e89",
            "85a34d30ab2443ed9feb325701ade596",
            "021a45ed513f40e5a30fd386c5973e9c",
            "1d197f0dd5bf40449b8328986e4df0a3",
            "77702880934e4ab4a4518631cde16aa5"
          ]
        },
        "outputId": "ef1c1d46-fcdb-479c-9af2-990a5bc311c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d895ea98f66e45df868895d3c2ff8552"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f429f430583a475e81f6030d4c98c959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:convert:Loading model file /content/drive/MyDrive/lab/gpu/hf/model.safetensors\n",
            "INFO:convert:model parameters count : 1100048384 (1B)\n",
            "INFO:convert:params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/content/drive/MyDrive/lab/gpu/hf'))\n",
            "INFO:convert:Loaded vocab file PosixPath('/content/drive/MyDrive/lab/gpu/hf/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
            "INFO:convert:Writing /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 2\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "INFO:convert:[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   1\n",
            "INFO:convert:[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "INFO:convert:[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "INFO:convert:[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
            "INFO:convert:[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
            "INFO:convert:[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
            "INFO:convert:[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   6\n",
            "INFO:convert:[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   7\n",
            "INFO:convert:[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "INFO:convert:[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "INFO:convert:[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  14\n",
            "INFO:convert:[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "INFO:convert:[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  16\n",
            "INFO:convert:[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  16\n",
            "INFO:convert:[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  17\n",
            "INFO:convert:[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  18\n",
            "INFO:convert:[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  21\n",
            "INFO:convert:[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  22\n",
            "INFO:convert:[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  24\n",
            "INFO:convert:[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  24\n",
            "INFO:convert:Wrote /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_2_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "5AtQ3OsSBUkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c16de39-df39-4af3-bf7e-a442194264bd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560689\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 2.05 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2098.35 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1260.23 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   191.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "I am not capable of providing any sample data. However, you can provide you with the information on how to implement it. The error in this information about the program code and the following the necessary instructions for the project to write the code for an example. \n",
            "\n",
            "the program as shown in the necessary information. Here is the structure and instructions on how to check the template can be a template for the following steps: \n",
            "\n",
            "as a tool you can perform\n",
            "llama_print_timings:        load time =    2546.89 ms\n",
            "llama_print_timings:      sample time =       5.49 ms /   100 runs   (    0.05 ms per token, 18218.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    8718.21 ms /   100 runs   (   87.18 ms per token,    11.47 tokens per second)\n",
            "llama_print_timings:       total time =    8792.19 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Challenge\n"
      ],
      "metadata": {
        "id": "6ojyBFyM_8ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./quantize {model_2_path} {model_3_path} Q8_0"
      ],
      "metadata": {
        "id": "xUxvn1aQA94K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d805ff-5f5f-4403-f7b3-bc6d81279e33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf' to '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  2098.35 MB\n",
            "llama_model_quantize_internal: quant size  =  1114.91 MB\n",
            "\n",
            "main: quantize time = 16431.29 ms\n",
            "main:    total time = 16431.29 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_3_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDa-xe6v9hKk",
        "outputId": "42864516-26c7-4935-e489-9c75b56e66d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560801\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q8_0:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 1.09 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  1114.91 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   669.61 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   136.91 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "INSERT INTO \"user\" (\"id\", \"first_name\", \"last_name\", \"last_name\", \"email\", \"age\", \"age\", \"age\", \"gender\") VALUES ('Johanna\", \"John\", \"John\", \"John\", \"Johanna\", \"John\")\n",
            "VALUES (1, \"John\", \"John\", \"Jane\", \"Jane\")\n",
            "VALUES (1\", \"John\", 50\n",
            "llama_print_timings:        load time =    1856.26 ms\n",
            "llama_print_timings:      sample time =       4.88 ms /   100 runs   (    0.05 ms per token, 20504.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6829.98 ms /   100 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
            "llama_print_timings:       total time =    6906.89 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Challenge\n"
      ],
      "metadata": {
        "id": "eBOHDrHL_dQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i_range=tqdm(range(10))\n",
        "for i in i_range:\n",
        "  problem = \"\\\"\" + problem_set[i] + \"\\\"\"\n",
        "\n",
        "  #model 1 (q4)\n",
        "  !./main -m {model_1_path} > {model_1_output_path} --n-predict 100 --n-gpu-layers 15 -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 2 (f16)\n",
        "  !./main -m {model_2_path} > {model_2_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 3 (q8)\n",
        "  !./main -m {model_3_path} > {model_3_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  open(path, 'w').close()\n",
        "  f = open(path, \"a\")\n",
        "  f.write(\"model 1 (q4)\\n\")\n",
        "  model_file = open(model_1_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 2 (f16)\\n\")\n",
        "  model_file = open(model_2_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 3 (q8)\\n\")\n",
        "  model_file = open(model_3_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCimame_iGh",
        "outputId": "e51a3fe3-889f-4333-d7f2-0692f3b07bc2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:45<00:00, 28.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  f = open(path, \"r\")\n",
        "  print(\"Problem: \",problem_set[i],\"\\n\")\n",
        "  print(f.read())\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJm0b8xL_k1w",
        "outputId": "67e6c047-e8b4-4024-ccb6-55800d4ffc41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem:  Explain the concept of gravity. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Explain the concept of gravity. What are the different types of gravity?\n",
            "Gravitational force and how it is measured?\n",
            "How does gravity? Explain the concept? Explain the concept of gravity?\n",
            "Can you explain the concept of gravity, as a force in physics? What is gravity?\n",
            "Tell me about the concept of gravity?\n",
            "What is gravity? Explained how?\n",
            "How are the concept of gravity and what is explained in every day? Explain the concept of gravity? Explain\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Explain the concept of gravity.\n",
            "Gravity is the force of attraction between objects, and how it affects the planets and its effects on the earth's motion, such as the moon, and its movement in space, the moon's movement in the earth, such as a force and the gravity.\n",
            "In this planet, and its effects, as if the motion of the moon and the moon is the moon and its relationship and its orbit.\n",
            "Gravity?\n",
            "Air, and its orbit\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Explain the concept of gravity.\n",
            "\n",
            "Explain the concept of gravity with examples.\n",
            "Gravity is a force, as shown through science, like a force of an object is a force and explain its effects, such as the concept, including:\n",
            "\n",
            "Gravity, it is a force of attraction, gravity, is the force, which is the effect of gravity, a force. Gravity is the force, as an object, it is one of, and it is the force, and it is a\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Continue this conversation: So, do you think it will rain tomorrow? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow?\n",
            "I'll be back at 11:00 tomorrow?\n",
            "I will be back tomorrow.\n",
            "You will be at 5:40a.\n",
            "You should see the rain tomorrow.\n",
            "Better tomorrow. Can you find tomorrow?\n",
            "I'll be back? I've been on a little later?\n",
            "Tomorrow morning. I will come back?\n",
            "What is your friend tomorrow.\n",
            "Tomorrow? Do you come back tom\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow?\n",
            "2. It's going to rain tomorrow.\n",
            "The sun is going to rain.\n",
            "The day, so rain today?\n",
            "Do you have to buy tomorrow, today?\n",
            "the rain or tomorrow?\n",
            "I'm in the weekend?\n",
            "How about tomorrow, so far, or raining today?\n",
            "the rain or tomorrow, but we'll be sure to the day.\n",
            "Today!\n",
            "The rain.\n",
            "I'll see\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow?\n",
            "Sure, I'm going to go to the beach tomorrow.\n",
            "Sure! Continue the meeting tomorrow.\n",
            "\n",
            "Because we'll do it? Continue the discussion!\n",
            "\n",
            "I don'll be in the car Continue working on the weekend.\n",
            "You know, I'll be in the Continue the next, we're going to the continuing this summer, and I'm looking forward to the rain!\n",
            "Continue to\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the derivative of x^2+3x+2? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What is the derivative of x^2+3x+2?\n",
            "2+2?\n",
            "\n",
            "\n",
            "A: Note that since the derivative of \n",
            "$$\n",
            "x^2+2=2\n",
            "=2=2+2\n",
            "$$\n",
            "\n",
            "The derivative of x=2\n",
            "\n",
            "A: The derivative is the answer \n",
            "so the derivative is : \n",
            "\n",
            "$$\n",
            "\\frac{2\n",
            "= x+2\n",
            "$$\n",
            "$$\n",
            "$$\n",
            "\n",
            "So the derivative of x=2\n",
            "\n",
            "$$\n",
            "$$\n",
            "\n",
            "What is equal to\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the derivative of x^2+3x+2?\n",
            "\n",
            "- In the formula for the second derivative of cosine function of the function of 2. What is the derivative of 3?\n",
            "- 5? How to calculate the value of 2?\n",
            "- Of cosine function?\n",
            "- A function of the inverse of x?\n",
            "- The inverse function 3. Can you explain how to solve the second-order \n",
            "What is a quadratic curve?\n",
            "\n",
            "- How to calculate the second?\n",
            "- In 2\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the derivative of x^2+3x+2? How to calculate the slope of the slope-intercept form and y-intercept of a quadratic function of a quadratic function and what is the tangent line in the graph of the line? How to use a quadratic function? What is the slope? (quadratic equation? The line of a? What is a quadratic function? What is the quadratic function?\n",
            "\n",
            "I am a quadratic function of a line?\n",
            "\n",
            "Can you explain the equation? It is a quadratic? A linear\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Discuss the significance of the Lunar New Year in East Asian cultures. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures.\n",
            "Knowing the history of the origins of the traditions, the Lunar New Years in East Asian cultures in terms of religion and how Chinese and how they origin and the customs, customs and celebrate in the tradition.\n",
            "Know the history of the traditional Chinese New Years and the holidays, it's origins of the traditional holidays in China as well as traditions, and to write an essay on the date, the new year\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures.comparative literature essay\n",
            "2015, Essay, Essay, Essay about the moon, Essay on the Lunar New Year's Day 2014, Essay, Essay on the Moon, Essay 1994, Lunar New Year, Essay on the New Year's Day, Essay, Essay, Fashion, Furniture, Essay, Focus, Essay, Lunar\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures.comparison of the traditional and contemporary dance performance in East Asian art, its role as well as theater and modern dance, in China, and how theater in performance arts is performed in contemporary art in modern art. This play of theater, and theater, and modern. As a social sciences in. Incorporate traditional art, and how to theater, dance theater, and cultural and performance studies, a play, as a cultural performance in theater.\n",
            "Afr\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Describe how a blockchain works. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Describe how a blockchain works.\n",
            "What is a smart contract and what is it?\n",
            "What are some of the key features and what makes blockchain technology used in blockchain technology?\n",
            "How is it different from a cryptocurrency?\n",
            "What is a blockchain?\n",
            "Why would I should I use blockchain?\n",
            "What is a cryptocurrency?\n",
            "How does it different from a blockchain\n",
            "Blockchain?\n",
            "What is blockchain?\n",
            "What is the difference between a blockchain?\n",
            "How does it\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Describe how a blockchain works.comparison of bitcoin works with other cryptocurrencies and how it is different from bitcoin, including its technology and its features, such as its blockchain technology.comparison to bitcoin is.\n",
            "blockchain and how it is different from other digital currencies like the bitcoin, like it, and how it works and how it functions, bitcoin's, bitcoin in terms of the technology, it's value in the blockchain.\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Describe how a blockchain works.b.compares to bitcoin's blockchain:\n",
            "\n",
            "1. Transactions.org and how bitcoin's blockchain works.org: a public blockchain's blockchain's technology is similar to bitcoin, but it's blockchain's technology is an open-blockchain (bitcoin and its distributed ledger technology, and bitcoin's blockchain. The blockchain, and bitcoin's ledgers\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\n",
            "Hazardous driving will be a big topic of the future?\n",
            "What about pedestrians, what about pedestrians?\n",
            "Will cars that get in the future?\n",
            "Does the future, do you think the autonomous cars?\n",
            "Does the future of self driving\n",
            "Self driving cars\n",
            "Do you think that self-driving cars\n",
            "By the future?\n",
            "In the autonomous\n",
            "Self driving\n",
            "Why it's coming in 2022?\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not? How is the need for autonomous vehicles? This is the most important?\n",
            "- Do you think self-driving cars? It’s a question of the road accidents, the safety?\n",
            "\n",
            "16: How to make the self-driving cars? It is essential to the drivers, how to paying is for self-driving car? Drivers are, car drivers are the most popular?\n",
            "\n",
            "the self-driving cars, self\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\n",
            "\n",
            "Driving a human? \n",
            "- Yes, yes, in some autonomous cars do you believe they are a self-driving cars can be a pedestrians and that is an essential. They should be the main to avoid the cars and safety drivers\n",
            "\n",
            "Self-driving cars and pedestrian, we are self-driving cars are a car-drivers. It's just that the most self-driving are the self\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Write a short story about a detective solving a mystery in a futuristic city. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city.\n",
            "Writing prompt: A detective solving a crime and how to write a mystery that heinrich german crime scene, 1936 and how the story\n",
            "Sherlock holmes solving the crime. 366 words essay on the case. In the detective fiction is the case, the hammond's last mystery, a mystery, crime, heinlein\n",
            "In 2, a detective, the man who solved the murder and the\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city. 1. The story that takes place in a futuristic crime scene, where the detective must solve a case of the future, and the case and science fiction, but the. The detective and is a crime fiction, which is a detective story in the mystery. The detective investigates the mystery is a crime is the key to solve a story of the detective who has to a mysterious case the real. The novel in which is the key in the detective,\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city. The detective who solves a case of the crime mystery, set in a futuristic mystery novel, and must solve a case, which involves a city in which the case and solve a puzzle, a mystery that involves a modern-style puzzle\n",
            "- solve a crime mystery in a case. Heist: The detective that leads to a case, but he solves, they have a mystery story that they solve a case is a complex. Detective's, solve the case\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Translate 'Hello, how are you?' into Japanese. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Translate 'Hello, how are you?' into Japanese.\n",
            "Hello, I am the most commonly used for 'Hello' in Japanese.\n",
            "How do you say 'How are you?' in Japanese in Japanese.\n",
            "Here are some expressions\n",
            "Hello in Japanese.\n",
            "Hello in Japanese, in English\n",
            "Hello\n",
            "Here is 'Hello.\n",
            "Hello.\n",
            "How are you.\n",
            "How are you doing.\n",
            "I do you.\n",
            "How can you.\n",
            "Hello. \n",
            "How are you?\n",
            "The translation\n",
            "Please tell me, please?\n",
            "\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Translate 'Hello, how are you?' into Japanese.\n",
            "\n",
            "Japanese:\n",
            "\"Yo, Koko-san\"\n",
            "\n",
            "Kanko:\n",
            "\"Konnichiwa\"\n",
            "\n",
            "Jakkoi ka?\n",
            "Ja ka?\n",
            "\n",
            "\"Konnichiwa\"\n",
            "Soshite, desu\n",
            "Mesho\n",
            "\n",
            "Kon\n",
            "\n",
            "Akuma-san\n",
            "\"Narumi desu\n",
            "Nani wa kokoro\n",
            "Hey\n",
            "Soshite\n",
            "Shikarama\n",
            "N\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Translate 'Hello, how are you?' into Japanese.\n",
            "\n",
            "- Translate the sentence: 「おはようございます」 into Japanese: はいです。 こんにちは、こんなんですように、しばしば、はじまってみます。\n",
            "\n",
            "こんばんはご過ごします。\n",
            "\n",
            "- よります。こんなどういまいりんごはん\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Tell a joke about artificial intelligence. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Tell a joke about artificial intelligence.\n",
            "1. A person asks an AI, \"What is AI and AI, what's the best programming language do you, the best language?\"\n",
            "2. The AI:\n",
            "\"Do you like, \"To what language?\n",
            "The best language?\"\n",
            "2\n",
            "The AI: \"How to build best for me, programming language, how to create AI.\"\n",
            "\"The best language?\n",
            "The answer: Python\"\n",
            "The AI: \"Best answer\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Tell a joke about artificial intelligence.\n",
            "\n",
            "5. \"The Art of AI-Generated Artificial Intelligence and the Future\" by Narrative: Infinite: AI: Inventor: AI vs. \"The Swear by Joker, a robotics engineer and AI: It's Realism\" by Ian Bogosian and the Future of the Future by Drone.com (2014\n",
            "\n",
            "6. \"Artificial Intelligence\" by Tom and the W\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Tell a joke about artificial intelligence.\n",
            "\n",
            "2. \"I have a new toy, and I am a robot, and he is just a bot, and I'm here to teach you how to dance,\" as told a robot.\n",
            "\n",
            "3. The robot.\n",
            "\n",
            "The robots are learning to talk to us.\n",
            "\n",
            "4. There'm not only a joke about a robot.\n",
            "\n",
            "5. That's like a joke, but a joke, a new one's.\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What are the implications of the latest financial news from Wall Street? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "13.12.2020 2020 G20's announcing the \"Rockefeller Foundation's investments and their stock market and its future incentives, and I am currently a new trends in the globalization of finance?\n",
            "12.24.18 2020. It has a 50,000 words and the global financial, \"P. F.\n",
            "25\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "2019-04-03\n",
            "- The U.S. Economic forecast, 202106/03/15%2C 2021-07-13-013/45%2016%29\n",
            "The 05-years-0.4% (Apr.32320\n",
            "Midnight-7 5-1/8%\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}