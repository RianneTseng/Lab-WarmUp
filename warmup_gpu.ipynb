{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8As5blYmviX8FtDGEJsyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d895ea98f66e45df868895d3c2ff8552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be035c18730c4b6b83ade9285e7e2250",
              "IPY_MODEL_4c588c55335a4f818dc225686df0ac03",
              "IPY_MODEL_647445d09c1a423e9301af0bab351beb"
            ],
            "layout": "IPY_MODEL_85ccaee92048447a84841fd549bd9947"
          }
        },
        "be035c18730c4b6b83ade9285e7e2250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102bc15118e44365927e2b919c164a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_b36cee81e683402a95be27d060cc3665",
            "value": "Fetching 10 files: 100%"
          }
        },
        "4c588c55335a4f818dc225686df0ac03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7157ddd184154c308045a8b82d4b42c8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66a4cdd4988449d79f1ec6053ab353e4",
            "value": 10
          }
        },
        "647445d09c1a423e9301af0bab351beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863b30347ca943e8a6022e23a8ef3aac",
            "placeholder": "​",
            "style": "IPY_MODEL_932908a370684392a398f9955338ede3",
            "value": " 10/10 [00:00&lt;00:00,  8.15it/s]"
          }
        },
        "85ccaee92048447a84841fd549bd9947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102bc15118e44365927e2b919c164a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36cee81e683402a95be27d060cc3665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7157ddd184154c308045a8b82d4b42c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a4cdd4988449d79f1ec6053ab353e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "863b30347ca943e8a6022e23a8ef3aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932908a370684392a398f9955338ede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f429f430583a475e81f6030d4c98c959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1946c169e68f4d91b7c248b27ac80009",
              "IPY_MODEL_481f027208344e879dcffbd0981c6a4e",
              "IPY_MODEL_c733a65d9ede4ff6b9ed40dc07d29cb1"
            ],
            "layout": "IPY_MODEL_ea21f5cb61284d478d1c5f490bb50bc0"
          }
        },
        "1946c169e68f4d91b7c248b27ac80009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3261e6e59674b15a516e431663d7522",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf0f70b6db1407c887daf02d6017e89",
            "value": "tokenizer.json: 100%"
          }
        },
        "481f027208344e879dcffbd0981c6a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a34d30ab2443ed9feb325701ade596",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_021a45ed513f40e5a30fd386c5973e9c",
            "value": 1842767
          }
        },
        "c733a65d9ede4ff6b9ed40dc07d29cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d197f0dd5bf40449b8328986e4df0a3",
            "placeholder": "​",
            "style": "IPY_MODEL_77702880934e4ab4a4518631cde16aa5",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 4.45MB/s]"
          }
        },
        "ea21f5cb61284d478d1c5f490bb50bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3261e6e59674b15a516e431663d7522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf0f70b6db1407c887daf02d6017e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85a34d30ab2443ed9feb325701ade596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021a45ed513f40e5a30fd386c5973e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d197f0dd5bf40449b8328986e4df0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77702880934e4ab4a4518631cde16aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpjssr/Lab-WarmUp/blob/main/warmup_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
        "#!sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "#!sudo apt-get update\n",
        "#!sudo apt-get -y install cuda-toolkit-12-5"
      ],
      "metadata": {
        "id": "_MIe4F4dBQ6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8I5QyeSCAJLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433f3819-6d10-44aa-bed8-e049f5811586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import snapshot_download\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ZOM6V_stP9rc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"/content/drive/MyDrive/lab/gpu\"\n",
        "llama_folder = root_folder + \"/llama.cpp\"\n",
        "hf_folder = root_folder + \"/hf\"\n",
        "output_folder = root_folder + \"/output\"\n",
        "\n",
        "main_path = llama_folder + \"/main\"\n",
        "quantize_path = llama_folder + \"/quantize\"\n",
        "\n",
        "hf_token=\"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\"\n",
        "gguf_model_name = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\"\n",
        "hf_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_1_id = \"tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\"\n",
        "model_2_hf = root_folder + \"/hf\"\n",
        "model_2_id = \"TinyLlama/\" + hf_model_name\n",
        "model_2_path = root_folder + \"/\" + hf_model_name + \"-f16.gguf\"\n",
        "model_3_path = root_folder + \"/\" + hf_model_name + \"-Q8.gguf\"\n",
        "model_1_output_path = output_folder + \"/model_1.txt\"\n",
        "model_2_output_path = output_folder + \"/model_2.txt\"\n",
        "model_3_output_path = output_folder + \"/model_3.txt\"\n",
        "\n",
        "problem_set=[\n",
        "\"What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa\",\n",
        "\"What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request\",\n",
        "\"Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter\",\n",
        "\"Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling\",\n",
        "\"What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990\",\n",
        "\"Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen\",\n",
        "\"Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle\",\n",
        "\"Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei\",\n",
        "\"What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity\",\n",
        "\"What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen\"]"
      ],
      "metadata": {
        "id": "GlKPcJhwP--g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd {llama_folder}\n",
        "!chmod 755 {main_path}\n",
        "!chmod 755 {quantize_path}"
      ],
      "metadata": {
        "id": "CYxCWZaqBIDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b26526-3930-45a1-af8a-991e1bddf8d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/lab/gpu/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Challenge"
      ],
      "metadata": {
        "id": "6co6DtiAUpDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_path = hf_hub_download(\n",
        "               filename=model_1_id,\n",
        "               local_dir=root_folder,\n",
        "               token=hf_token,\n",
        "               repo_id=gguf_model_name)"
      ],
      "metadata": {
        "id": "O5v3L_dwAxrq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_1_path} --n-predict 100 --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "A647sdgeBL5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9be1385-cdff-42e5-a354-558724821299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560650\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_K:  135 tensors\n",
            "llama_model_loader: - type q6_K:   21 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 262/32003 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32003\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   375.36 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   117.78 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s>ints of a deafening noise. The screaming stopped. \"Oh, what? What is that?\"\n",
            "Jin-Rohan's voice was barely a few seconds later.\n",
            "\"What is that?\"\n",
            "\"I told you, what is that!\"\n",
            "Jin-chan was a screamed as he didn't even heard as he could barely hear a loud, \"What noise?\"\n",
            "He looked at the sound of the noise. \"Shit\n",
            "llama_print_timings:        load time =    2075.22 ms\n",
            "llama_print_timings:      sample time =       5.33 ms /   100 runs   (    0.05 ms per token, 18775.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    5172.27 ms /   100 runs   (   51.72 ms per token,    19.33 tokens per second)\n",
            "llama_print_timings:       total time =    5249.58 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Medium Challenge"
      ],
      "metadata": {
        "id": "VyOv0Z97TcEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_download(\n",
        "    repo_id=model_2_id,\n",
        "    local_dir=model_2_hf,\n",
        "    local_dir_use_symlinks=False,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=hf_token)\n",
        "!python convert.py {hf_folder} --outfile {model_2_path} --outtype f16"
      ],
      "metadata": {
        "id": "LYDmuMZzA1eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d895ea98f66e45df868895d3c2ff8552",
            "be035c18730c4b6b83ade9285e7e2250",
            "4c588c55335a4f818dc225686df0ac03",
            "647445d09c1a423e9301af0bab351beb",
            "85ccaee92048447a84841fd549bd9947",
            "102bc15118e44365927e2b919c164a3b",
            "b36cee81e683402a95be27d060cc3665",
            "7157ddd184154c308045a8b82d4b42c8",
            "66a4cdd4988449d79f1ec6053ab353e4",
            "863b30347ca943e8a6022e23a8ef3aac",
            "932908a370684392a398f9955338ede3",
            "f429f430583a475e81f6030d4c98c959",
            "1946c169e68f4d91b7c248b27ac80009",
            "481f027208344e879dcffbd0981c6a4e",
            "c733a65d9ede4ff6b9ed40dc07d29cb1",
            "ea21f5cb61284d478d1c5f490bb50bc0",
            "c3261e6e59674b15a516e431663d7522",
            "dcf0f70b6db1407c887daf02d6017e89",
            "85a34d30ab2443ed9feb325701ade596",
            "021a45ed513f40e5a30fd386c5973e9c",
            "1d197f0dd5bf40449b8328986e4df0a3",
            "77702880934e4ab4a4518631cde16aa5"
          ]
        },
        "outputId": "ef1c1d46-fcdb-479c-9af2-990a5bc311c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d895ea98f66e45df868895d3c2ff8552"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f429f430583a475e81f6030d4c98c959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:convert:Loading model file /content/drive/MyDrive/lab/gpu/hf/model.safetensors\n",
            "INFO:convert:model parameters count : 1100048384 (1B)\n",
            "INFO:convert:params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/content/drive/MyDrive/lab/gpu/hf'))\n",
            "INFO:convert:Loaded vocab file PosixPath('/content/drive/MyDrive/lab/gpu/hf/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
            "INFO:convert:Writing /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 2\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "INFO:convert:[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   1\n",
            "INFO:convert:[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "INFO:convert:[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "INFO:convert:[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
            "INFO:convert:[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
            "INFO:convert:[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
            "INFO:convert:[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   6\n",
            "INFO:convert:[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   7\n",
            "INFO:convert:[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "INFO:convert:[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "INFO:convert:[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  14\n",
            "INFO:convert:[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "INFO:convert:[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  16\n",
            "INFO:convert:[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  16\n",
            "INFO:convert:[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  17\n",
            "INFO:convert:[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  18\n",
            "INFO:convert:[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  21\n",
            "INFO:convert:[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  22\n",
            "INFO:convert:[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  24\n",
            "INFO:convert:[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  24\n",
            "INFO:convert:Wrote /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_2_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "5AtQ3OsSBUkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c16de39-df39-4af3-bf7e-a442194264bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560689\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 2.05 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2098.35 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1260.23 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   191.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "I am not capable of providing any sample data. However, you can provide you with the information on how to implement it. The error in this information about the program code and the following the necessary instructions for the project to write the code for an example. \n",
            "\n",
            "the program as shown in the necessary information. Here is the structure and instructions on how to check the template can be a template for the following steps: \n",
            "\n",
            "as a tool you can perform\n",
            "llama_print_timings:        load time =    2546.89 ms\n",
            "llama_print_timings:      sample time =       5.49 ms /   100 runs   (    0.05 ms per token, 18218.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    8718.21 ms /   100 runs   (   87.18 ms per token,    11.47 tokens per second)\n",
            "llama_print_timings:       total time =    8792.19 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Challenge\n"
      ],
      "metadata": {
        "id": "6ojyBFyM_8ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./quantize {model_2_path} {model_3_path} Q8_0"
      ],
      "metadata": {
        "id": "xUxvn1aQA94K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d805ff-5f5f-4403-f7b3-bc6d81279e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf' to '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  2098.35 MB\n",
            "llama_model_quantize_internal: quant size  =  1114.91 MB\n",
            "\n",
            "main: quantize time = 16431.29 ms\n",
            "main:    total time = 16431.29 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_3_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDa-xe6v9hKk",
        "outputId": "42864516-26c7-4935-e489-9c75b56e66d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560801\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q8_0:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 1.09 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  1114.91 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   669.61 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   136.91 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "INSERT INTO \"user\" (\"id\", \"first_name\", \"last_name\", \"last_name\", \"email\", \"age\", \"age\", \"age\", \"gender\") VALUES ('Johanna\", \"John\", \"John\", \"John\", \"Johanna\", \"John\")\n",
            "VALUES (1, \"John\", \"John\", \"Jane\", \"Jane\")\n",
            "VALUES (1\", \"John\", 50\n",
            "llama_print_timings:        load time =    1856.26 ms\n",
            "llama_print_timings:      sample time =       4.88 ms /   100 runs   (    0.05 ms per token, 20504.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6829.98 ms /   100 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
            "llama_print_timings:       total time =    6906.89 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Challenge\n"
      ],
      "metadata": {
        "id": "eBOHDrHL_dQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i_range=tqdm(range(10))\n",
        "for i in i_range:\n",
        "  problem = \"\\\"\" + problem_set[i] + \" Answer: \\\"\"\n",
        "\n",
        "  #model 1 (q4)\n",
        "  !./main -m {model_1_path} > {model_1_output_path} --n-predict 100 --n-gpu-layers 15 -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 2 (f16)\n",
        "  !./main -m {model_2_path} > {model_2_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 3 (q8)\n",
        "  !./main -m {model_3_path} > {model_3_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  open(path, 'w').close()\n",
        "  f = open(path, \"a\")\n",
        "  f.write(\"model 1 (q4)\\n\")\n",
        "  model_file = open(model_1_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 2 (f16)\\n\")\n",
        "  model_file = open(model_2_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 3 (q8)\\n\")\n",
        "  model_file = open(model_3_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCimame_iGh",
        "outputId": "4225dd0d-b81c-4a9f-8281-33b7c4569757"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:22<00:00, 26.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  f = open(path, \"r\")\n",
        "  print(\"Problem: \",problem_set[i],\"\\n\")\n",
        "  print(f.read())\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJm0b8xL_k1w",
        "outputId": "a9f2a932-5e44-4fd7-d7f7-9fb3b390b58f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem:  What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa Answer: \n",
            "Québec\n",
            "What is the capital of Canada\n",
            "What is the capital of Canada\n",
            "What is the capital of Canada\n",
            "What is? Answer: Ottawa\n",
            "What is the capital of Canada?\n",
            "What is the capital of Canada\n",
            "What is the capital of Canada\n",
            "What is Canada?\n",
            "What is the capital of Canada?\n",
            "What state in Canada?\n",
            "What is the capital of Canada?\n",
            "What is\n",
            "What state in Canada\n",
            "What state\n",
            "What state in Canada\n",
            "What is\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa Answer: 6.\n",
            "Canada:\n",
            "3. Which city is the capital of Canada? 1. Ontario (Vancouver: Canada\n",
            "2. What is the capital of Canada? Vancouver\n",
            "What is the capital of British Columbia\n",
            "British Columbia: Vancouver?\n",
            "1 Quebec?\n",
            "Capital?\n",
            "(London\n",
            "(British Columbia: Vancouver?\n",
            "What is the capital of Canada: Ontario: Vancouver, Ontario\n",
            "Canada?\n",
            "Island?\n",
            "\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa Answer: \n",
            "\n",
            "What is the capital of Canada? Answer:\n",
            "The capital of Canada (Ontario (Ontario: Toronto) (Ontario (Ontario) (Ontario) \n",
            "\n",
            "The capital of Ontario, Canada: Ontario? (Ontario)\n",
            "(Ontario) (Ontario: Ontario) (Ontario) (Ontario) (Canada)\n",
            "\n",
            "Answer: Toronto) (Ontario) (Ontario\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request Answer: 432004\n",
            "http 405 error means that the server cannot handle the request.\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            "\n",
            "*The requested URL was not found for the page or is not public method\n",
            "\n",
            "*the method or resource:\n",
            "\n",
            "\n",
            "HTTP method not found\n",
            "The resource could not found.\n",
            "\n",
            "\n",
            "*Org4 HTTP method not found \n",
            "What is missing. The method\n",
            "The Apache Tomcat74\n",
            "\n",
            "What does this error message.\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request Answer: 01: Failed (?)\n",
            "? What is this?\n",
            "\n",
            "The HTTP error code (403) (HTTP0.1.4125)\n",
            "(Not found (4040) (Bad) (04) (Bad)\n",
            "Error:\n",
            "Bad (4)\n",
            "(Bad)\n",
            "1037) 5.246) \n",
            "(94) 2)\n",
            "\n",
            "196) 15, 1\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request Answer: 444\n",
            "<|assistant\n",
            "The 200245110, I200313| \n",
            "The HTTP 9,000 1721,000900-001 (28492|24-2, 141130|2014) 2012\n",
            "(5|0000|1-45|9\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter Answer: 0\n",
            "Tell me!\n",
            "There are many planets in our solar system is actually in the Saturn\n",
            "There are many moons!\n",
            "There is a planet\n",
            "There are many planets\n",
            "Planets are\n",
            "There are many planets are moons\n",
            "The most planets in the solar system\n",
            "There are many moons\n",
            "There are many moons in our solar system\n",
            "There are many moons in our solar system\n",
            "Planets are made of Jupiter\n",
            "Which of\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter Answer: \n",
            "\n",
            "The planet with the most diamond \n",
            "The planet that is the second largest moon\n",
            "\n",
            "Red planet in the solar system\n",
            "\n",
            "Alien: Mercury: The Moon, Jupiter\n",
            "Venus \n",
            "The planet is the smallest planet and Jupiter (Mercury\n",
            "Mercury: \n",
            "\n",
            "Solar System\n",
            "Mercury:\n",
            "The planet in the sky\n",
            "\n",
            "Planetarium:\n",
            "\n",
            "(Earth\n",
            "\n",
            "Mercury\n",
            "(\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter Answer: \n",
            "\n",
            "Venus\n",
            "Venus is the second planet\n",
            "Venus is the second planet: The first planet(Venus is a gas\n",
            "(Saturn \n",
            "Red: Venus is the planet:\n",
            "\n",
            "(Moon \n",
            "Earth is\n",
            "Moon\n",
            "\n",
            "The moon (A) is the planet is the planet in our planet (Saturn\n",
            "\n",
            "As the asteroid: The asteroid (Venus:\n",
            "\n",
            "Because it\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling Answer: 5?\n",
            "Would you say the Paper Paper, uhh, I mean I've read it?\n",
            "Hmm?\n",
            "\n",
            "A: This is a short answer to 0: 3: ?\n",
            "(\n",
            "\n",
            "\n",
            "W-\n",
            "32: 3 \n",
            "\n",
            "A: The \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I have a bit of a little something, the answer is no, not here, 2:\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling Answer: 3 (1)15 - a.\n",
            "The Life 8940 and Huxley (3) - The Great - Gates - the19366047; and the Painter1. 4 (5)1. The F. And F (2.2 6,310 (2000. 1937, 123611)4604, The In196\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling Answer: 10709 (19819423394 (97812951118.43-8760, a8 345148,030-69628?\n",
            "2004) 158942182032,07 (31.7000 and 141618018)\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990 \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990 Answer: 195261118 1978\n",
            "Democrats40121018.07 46.11 08/04:11:50:31:19:38 13:57:02:018.1\n",
            "In 1700.97:65 11:21:29:49:31\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990 Answer: 1310 (A19: The Wall:82389 706997 (88/91000000.90)96908.9 978:50447:00:11000/905.02:10230:2/1200:1:1024: 9.1:1\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990 Answer: 9 129 (4?161902091.\n",
            "-1102?23012 (3,000.98:908395)\n",
            "1:100086)3 (2006)909,0004012 (B01939:9:96:30: 79:7985?\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen Answer: 4lJEA11n's. 1979.\n",
            "EU 14 (3 50c)l0e1 1e-751t? 1975, 19. 5486-89764B67; 263.\n",
            "in 41 14 1; 10 4. 1914aB, 8\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen Answer: \n",
            "18. Which elements have a molecules?\n",
            "(s)\n",
            "(C) (E)\n",
            "1. A) A) (Phenyl) (B) (Alkyl) (Alkyl)\n",
            "(1) (Alcohol)\n",
            "A) Alpha/N)\n",
            "2) (A) (Alkyl) (Gen) (D)\n",
            "12) Acyclohexyl (Rosylinesulfamate) (\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen Answer: \n",
            "-D) (1)Hydrogen (12O3(001,N (I)\n",
            "-10. Anion (Oxide:O2, (1, Oxygen)\n",
            "B) (15)\n",
            "-1)\n",
            "-42, H2 (7, N, 1)\n",
            "N (16, (H2, O2,1,1401)\n",
            "-1.0-613\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle Answer: 134 231; t4985l;A8) 11(Ja1337001 1821 17110324124; 103002;)2010-028040703721495789 064676100000022;12\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle Answer: 2016) the.\n",
            "I was, that he. I will I can the (talking his) the an the A/The in this, the, so I. The (f the the \" the I is to, to-s a.\n",
            "the? A, to? It's a, I's (the to, the, the to: for 1986 the, the, the (i I and.\n",
            "The (and and\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle Answer: 200) 1500s) (19th-2008: 500;101982013003.617/9.\n",
            "The book of 19135:10 24690578:223;258-12,02.5307 and 8,7463505:00\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei Answer: \n",
            "Tell me the difference between the theory and Einstein (says in the theory of relativity, he's right.\n",
            "Is the theory of relativity the theory is not an exception to the theory of relativity\n",
            "\n",
            "and the theory of relativity\n",
            "It is the theory\n",
            "Gravitational field\n",
            "\n",
            "1926\n",
            "Through the theory\n",
            "\n",
            "The theory of 1905\n",
            "\n",
            "theorizes gravity (spacetime 17\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei Answer: 10015 Theoratico: a. 5.\n",
            "In 13133\n",
            "I 679190-5 41121654400 and 2,711.4307 and 19949594.1484) 12384299892.23455. 246671\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei Answer: 1, 1776-3: 's(d's (b) in 1967; Theology(s) (and GalileoG-e, 2010, 2017. 2011.68) 1918, 02806012. 38207;30(1)905002;, 20\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity Answer: 1501A 100? 1000?\n",
            "Dialing out 451(100 17000000000000012 12V, 96 10000 0 1200000000000000 172.0,0000000000000\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity Answer: 2, and the time-related.\n",
            "11.22 and time and is the speed (Based on) is 0 5.\n",
            "B. 2.152.318.228? 2.2 (15) is the data (F-1) in the size 1) 175.\n",
            "53? 51-1202? 01, 1950, \n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity Answer: 5% in the 3,000&? \n",
            "110936.660000 for 1. 17605300 8528 06. 125, 8053 1926-3,612? 103,180? 211 to 1250. \n",
            "\n",
            "17,000\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen Answer: 4 1132501455304, 1990s? 641761, 160140?\n",
            "Essay on the, and, 177, 540? 26,3-diiO 18877-9736307.\n",
            "WWW, 8380 395B433\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen Answer: \n",
            "1. (Jersey. Can you translate the book that your? What is my life? You are?)\n",
            "\n",
            "Doxygen, and also the in-form, I want to be a. The new (B.\n",
            "Got my essay essay, I will. The Oxygen. In that, my, and the (Aquatic (Oxygen (Oxygen, and the most (Hydrogen (Hydrogen)\n",
            "\n",
            "(C\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen Answer: \n",
            "I want to share with us a \n",
            "I'll say a)\n",
            "The oxygen (Nitrogen (air (dio) and its oxygen, that you \n",
            "it, I am that is a)gen (Nitro (e) (Hydrogen \n",
            "oxygen (oxygen) (it)\n",
            "\n",
            "Nitrogen:Ox)\n",
            "Nitrogen (gen) (air) (oil) (gen\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}