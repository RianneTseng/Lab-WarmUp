{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmGR5taLvWrBHYPo7TwiH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d93fdd92e98d46bd90f006a7354c9c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14673c3b6c6d479f8adb2f4cecc45aeb",
              "IPY_MODEL_dbbe535eefba4f4f9ca4d2c75aea9b75",
              "IPY_MODEL_4afcb255e6414b9197074fbb15ca96e5"
            ],
            "layout": "IPY_MODEL_2c485c1d12af4099b547d84a3925bcd3"
          }
        },
        "14673c3b6c6d479f8adb2f4cecc45aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f940839316634636acb419951124a24c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d1c0114c4304494817aadfaeca83760",
            "value": "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf: 100%"
          }
        },
        "dbbe535eefba4f4f9ca4d2c75aea9b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614e601219964e7fba6f94318d6a062c",
            "max": 667822976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e5b1e507403479e9641fdb35038c61c",
            "value": 667822976
          }
        },
        "4afcb255e6414b9197074fbb15ca96e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80bf01cb104840e680e73c16b9767ac1",
            "placeholder": "​",
            "style": "IPY_MODEL_e8cd72f8941c4ccfa30c54baee68c0f3",
            "value": " 668M/668M [00:03&lt;00:00, 222MB/s]"
          }
        },
        "2c485c1d12af4099b547d84a3925bcd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f940839316634636acb419951124a24c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d1c0114c4304494817aadfaeca83760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "614e601219964e7fba6f94318d6a062c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5b1e507403479e9641fdb35038c61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80bf01cb104840e680e73c16b9767ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8cd72f8941c4ccfa30c54baee68c0f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d895ea98f66e45df868895d3c2ff8552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be035c18730c4b6b83ade9285e7e2250",
              "IPY_MODEL_4c588c55335a4f818dc225686df0ac03",
              "IPY_MODEL_647445d09c1a423e9301af0bab351beb"
            ],
            "layout": "IPY_MODEL_85ccaee92048447a84841fd549bd9947"
          }
        },
        "be035c18730c4b6b83ade9285e7e2250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102bc15118e44365927e2b919c164a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_b36cee81e683402a95be27d060cc3665",
            "value": "Fetching 10 files: 100%"
          }
        },
        "4c588c55335a4f818dc225686df0ac03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7157ddd184154c308045a8b82d4b42c8",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66a4cdd4988449d79f1ec6053ab353e4",
            "value": 10
          }
        },
        "647445d09c1a423e9301af0bab351beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863b30347ca943e8a6022e23a8ef3aac",
            "placeholder": "​",
            "style": "IPY_MODEL_932908a370684392a398f9955338ede3",
            "value": " 10/10 [00:00&lt;00:00,  8.15it/s]"
          }
        },
        "85ccaee92048447a84841fd549bd9947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102bc15118e44365927e2b919c164a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36cee81e683402a95be27d060cc3665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7157ddd184154c308045a8b82d4b42c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a4cdd4988449d79f1ec6053ab353e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "863b30347ca943e8a6022e23a8ef3aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932908a370684392a398f9955338ede3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f429f430583a475e81f6030d4c98c959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1946c169e68f4d91b7c248b27ac80009",
              "IPY_MODEL_481f027208344e879dcffbd0981c6a4e",
              "IPY_MODEL_c733a65d9ede4ff6b9ed40dc07d29cb1"
            ],
            "layout": "IPY_MODEL_ea21f5cb61284d478d1c5f490bb50bc0"
          }
        },
        "1946c169e68f4d91b7c248b27ac80009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3261e6e59674b15a516e431663d7522",
            "placeholder": "​",
            "style": "IPY_MODEL_dcf0f70b6db1407c887daf02d6017e89",
            "value": "tokenizer.json: 100%"
          }
        },
        "481f027208344e879dcffbd0981c6a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a34d30ab2443ed9feb325701ade596",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_021a45ed513f40e5a30fd386c5973e9c",
            "value": 1842767
          }
        },
        "c733a65d9ede4ff6b9ed40dc07d29cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d197f0dd5bf40449b8328986e4df0a3",
            "placeholder": "​",
            "style": "IPY_MODEL_77702880934e4ab4a4518631cde16aa5",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 4.45MB/s]"
          }
        },
        "ea21f5cb61284d478d1c5f490bb50bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3261e6e59674b15a516e431663d7522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf0f70b6db1407c887daf02d6017e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85a34d30ab2443ed9feb325701ade596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021a45ed513f40e5a30fd386c5973e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d197f0dd5bf40449b8328986e4df0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77702880934e4ab4a4518631cde16aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpjssr/Lab-WarmUp/blob/main/warmup_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
        "#!sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
        "#!sudo apt-get update\n",
        "#!sudo apt-get -y install cuda-toolkit-12-5"
      ],
      "metadata": {
        "id": "_MIe4F4dBQ6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8I5QyeSCAJLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b907a4-fb7f-489b-a8d5-991e9402f795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=296593 sha256=bc338d8c1e29c997747e617dd081688253414b16843623975296ccbc0d5a62ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import snapshot_download\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ZOM6V_stP9rc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"/content/drive/MyDrive/lab/gpu\"\n",
        "llama_folder = root_folder + \"/llama.cpp\"\n",
        "hf_folder = root_folder + \"/hf\"\n",
        "output_folder = root_folder + \"/output\"\n",
        "\n",
        "main_path = llama_folder + \"/main\"\n",
        "quantize_path = llama_folder + \"/quantize\"\n",
        "\n",
        "hf_token=\"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\"\n",
        "gguf_model_name = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\"\n",
        "hf_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_1_id = \"tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\"\n",
        "model_2_hf = root_folder + \"/hf\"\n",
        "model_2_id = \"TinyLlama/\" + hf_model_name\n",
        "model_2_path = root_folder + \"/\" + hf_model_name + \"-f16.gguf\"\n",
        "model_3_path = root_folder + \"/\" + hf_model_name + \"-Q8.gguf\"\n",
        "model_1_output_path = output_folder + \"/model_1.txt\"\n",
        "model_2_output_path = output_folder + \"/model_2.txt\"\n",
        "model_3_output_path = output_folder + \"/model_3.txt\"\n",
        "\n",
        "problem_set=[\n",
        "\"What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa\",\n",
        "\"What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request\",\n",
        "\"Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter\",\n",
        "\"Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling\",\n",
        "\"What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990\",\n",
        "\"Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen\",\n",
        "\"Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle\",\n",
        "\"Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei\",\n",
        "\"What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity\",\n",
        "\"What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen\"]"
      ],
      "metadata": {
        "id": "GlKPcJhwP--g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd {llama_folder}\n",
        "!chmod 755 {main_path}\n",
        "!chmod 755 {quantize_path}"
      ],
      "metadata": {
        "id": "CYxCWZaqBIDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256fd129-bf1b-4004-eaed-5bd12b237a9d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/lab/gpu/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Challenge"
      ],
      "metadata": {
        "id": "6co6DtiAUpDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1_id)\n",
        "print(root_folder)\n",
        "print(gguf_model_name)\n",
        "model_1_path = hf_hub_download(\n",
        "               filename=model_1_id,\n",
        "               local_dir=root_folder,\n",
        "               token=hf_token,\n",
        "               repo_id=gguf_model_name)"
      ],
      "metadata": {
        "id": "O5v3L_dwAxrq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "d93fdd92e98d46bd90f006a7354c9c1a",
            "14673c3b6c6d479f8adb2f4cecc45aeb",
            "dbbe535eefba4f4f9ca4d2c75aea9b75",
            "4afcb255e6414b9197074fbb15ca96e5",
            "2c485c1d12af4099b547d84a3925bcd3",
            "f940839316634636acb419951124a24c",
            "7d1c0114c4304494817aadfaeca83760",
            "614e601219964e7fba6f94318d6a062c",
            "9e5b1e507403479e9641fdb35038c61c",
            "80bf01cb104840e680e73c16b9767ac1",
            "e8cd72f8941c4ccfa30c54baee68c0f3"
          ]
        },
        "outputId": "45d5d8e7-50c7-41bd-98de-b0f5c39387d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\n",
            "/content/drive/MyDrive/lab/gpu\n",
            "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf:   0%|          | 0.00/668M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d93fdd92e98d46bd90f006a7354c9c1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_1_path} --n-predict 100 --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "A647sdgeBL5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9be1385-cdff-42e5-a354-558724821299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560650\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_K:  135 tensors\n",
            "llama_model_loader: - type q6_K:   21 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 262/32003 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32003\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   375.36 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   117.78 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s>ints of a deafening noise. The screaming stopped. \"Oh, what? What is that?\"\n",
            "Jin-Rohan's voice was barely a few seconds later.\n",
            "\"What is that?\"\n",
            "\"I told you, what is that!\"\n",
            "Jin-chan was a screamed as he didn't even heard as he could barely hear a loud, \"What noise?\"\n",
            "He looked at the sound of the noise. \"Shit\n",
            "llama_print_timings:        load time =    2075.22 ms\n",
            "llama_print_timings:      sample time =       5.33 ms /   100 runs   (    0.05 ms per token, 18775.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    5172.27 ms /   100 runs   (   51.72 ms per token,    19.33 tokens per second)\n",
            "llama_print_timings:       total time =    5249.58 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Medium Challenge"
      ],
      "metadata": {
        "id": "VyOv0Z97TcEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_download(\n",
        "    repo_id=model_2_id,\n",
        "    local_dir=model_2_hf,\n",
        "    local_dir_use_symlinks=False,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=hf_token)\n",
        "!python convert.py {hf_folder} --outfile {model_2_path} --outtype f16"
      ],
      "metadata": {
        "id": "LYDmuMZzA1eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d895ea98f66e45df868895d3c2ff8552",
            "be035c18730c4b6b83ade9285e7e2250",
            "4c588c55335a4f818dc225686df0ac03",
            "647445d09c1a423e9301af0bab351beb",
            "85ccaee92048447a84841fd549bd9947",
            "102bc15118e44365927e2b919c164a3b",
            "b36cee81e683402a95be27d060cc3665",
            "7157ddd184154c308045a8b82d4b42c8",
            "66a4cdd4988449d79f1ec6053ab353e4",
            "863b30347ca943e8a6022e23a8ef3aac",
            "932908a370684392a398f9955338ede3",
            "f429f430583a475e81f6030d4c98c959",
            "1946c169e68f4d91b7c248b27ac80009",
            "481f027208344e879dcffbd0981c6a4e",
            "c733a65d9ede4ff6b9ed40dc07d29cb1",
            "ea21f5cb61284d478d1c5f490bb50bc0",
            "c3261e6e59674b15a516e431663d7522",
            "dcf0f70b6db1407c887daf02d6017e89",
            "85a34d30ab2443ed9feb325701ade596",
            "021a45ed513f40e5a30fd386c5973e9c",
            "1d197f0dd5bf40449b8328986e4df0a3",
            "77702880934e4ab4a4518631cde16aa5"
          ]
        },
        "outputId": "ef1c1d46-fcdb-479c-9af2-990a5bc311c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d895ea98f66e45df868895d3c2ff8552"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f429f430583a475e81f6030d4c98c959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:convert:Loading model file /content/drive/MyDrive/lab/gpu/hf/model.safetensors\n",
            "INFO:convert:model parameters count : 1100048384 (1B)\n",
            "INFO:convert:params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/content/drive/MyDrive/lab/gpu/hf'))\n",
            "INFO:convert:Loaded vocab file PosixPath('/content/drive/MyDrive/lab/gpu/hf/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
            "INFO:convert:Writing /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 2\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "INFO:convert:[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   1\n",
            "INFO:convert:[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "INFO:convert:[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "INFO:convert:[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "INFO:convert:[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
            "INFO:convert:[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
            "INFO:convert:[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
            "INFO:convert:[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
            "INFO:convert:[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   6\n",
            "INFO:convert:[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   7\n",
            "INFO:convert:[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "INFO:convert:[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "INFO:convert:[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  14\n",
            "INFO:convert:[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "INFO:convert:[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  16\n",
            "INFO:convert:[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  16\n",
            "INFO:convert:[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  17\n",
            "INFO:convert:[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  18\n",
            "INFO:convert:[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "INFO:convert:[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  21\n",
            "INFO:convert:[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  22\n",
            "INFO:convert:[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  24\n",
            "INFO:convert:[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  24\n",
            "INFO:convert:Wrote /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_2_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "id": "5AtQ3OsSBUkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c16de39-df39-4af3-bf7e-a442194264bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560689\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 2.05 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2098.35 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  1260.23 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   191.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "I am not capable of providing any sample data. However, you can provide you with the information on how to implement it. The error in this information about the program code and the following the necessary instructions for the project to write the code for an example. \n",
            "\n",
            "the program as shown in the necessary information. Here is the structure and instructions on how to check the template can be a template for the following steps: \n",
            "\n",
            "as a tool you can perform\n",
            "llama_print_timings:        load time =    2546.89 ms\n",
            "llama_print_timings:      sample time =       5.49 ms /   100 runs   (    0.05 ms per token, 18218.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    8718.21 ms /   100 runs   (   87.18 ms per token,    11.47 tokens per second)\n",
            "llama_print_timings:       total time =    8792.19 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Challenge\n"
      ],
      "metadata": {
        "id": "6ojyBFyM_8ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./quantize {model_2_path} {model_3_path} Q8_0"
      ],
      "metadata": {
        "id": "xUxvn1aQA94K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d805ff-5f5f-4403-f7b3-bc6d81279e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf' to '/content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  2098.35 MB\n",
            "llama_model_quantize_internal: quant size  =  1114.91 MB\n",
            "\n",
            "main: quantize time = 16431.29 ms\n",
            "main:    total time = 16431.29 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m {model_3_path} --n-predict 100 --ignore-eos --n-gpu-layers 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDa-xe6v9hKk",
        "outputId": "42864516-26c7-4935-e489-9c75b56e66d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716560801\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/gpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q8_0:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 1.09 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
            "llm_load_tensors: offloading 15 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 15/23 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  1114.91 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =   669.61 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =     3.50 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =     7.50 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   136.91 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 81\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "INSERT INTO \"user\" (\"id\", \"first_name\", \"last_name\", \"last_name\", \"email\", \"age\", \"age\", \"age\", \"gender\") VALUES ('Johanna\", \"John\", \"John\", \"John\", \"Johanna\", \"John\")\n",
            "VALUES (1, \"John\", \"John\", \"Jane\", \"Jane\")\n",
            "VALUES (1\", \"John\", 50\n",
            "llama_print_timings:        load time =    1856.26 ms\n",
            "llama_print_timings:      sample time =       4.88 ms /   100 runs   (    0.05 ms per token, 20504.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6829.98 ms /   100 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
            "llama_print_timings:       total time =    6906.89 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Challenge\n"
      ],
      "metadata": {
        "id": "eBOHDrHL_dQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i_range=tqdm(range(10))\n",
        "for i in i_range:\n",
        "  problem = \"\\\"\" + problem_set[i] + \"\\\"\"\n",
        "\n",
        "  #model 1 (q4)\n",
        "  !./main -m {model_1_path} > {model_1_output_path} --n-predict 100 --n-gpu-layers 15 -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 2 (f16)\n",
        "  !./main -m {model_2_path} > {model_2_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 3 (q8)\n",
        "  !./main -m {model_3_path} > {model_3_output_path} --n-predict 100 --n-gpu-layers 15 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  open(path, 'w').close()\n",
        "  f = open(path, \"a\")\n",
        "  f.write(\"model 1 (q4)\\n\")\n",
        "  model_file = open(model_1_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 2 (f16)\\n\")\n",
        "  model_file = open(model_2_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 3 (q8)\\n\")\n",
        "  model_file = open(model_3_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCimame_iGh",
        "outputId": "faec3c1e-8b43-49a9-87de-879814ee98cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:04<00:00, 24.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  f = open(path, \"r\")\n",
        "  print(\"Problem: \",problem_set[i],\"\\n\")\n",
        "  print(f.read())\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJm0b8xL_k1w",
        "outputId": "abc7e7d8-0f04-4c1d-ec6f-19b245011b0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem:  What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa\n",
            "Canada: The capital of Canada?\n",
            "Answer: The capital of Canada\n",
            "3. Which country is known as the world's largest city? Answer: Montreal (Canada: Canada?s capital?\n",
            "Montreal (Canada?s city is the largest city?\n",
            "Answer: Canada?s most populous city?\n",
            "1. The United States of Canada?\n",
            "Canada?s largest city?\n",
            "Canada?s capital?\n",
            "2. Montreal?\n",
            "(O\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the capital of Canada? (A) Vancouver (B) Toronto (C) Montreal (D) Ottawa\n",
            "Canada: Toronto (Ontario (Ontario)\n",
            "Answer: Ontario\n",
            "2. Who is the capital city of Ontario (Ontario (Ontario)\n",
            "Canada: Toronto (Ontario)\n",
            "Capital: Toronto (Ontario)\n",
            "(Ontario (Ontario) (Ontario)\n",
            "Senate: Toronto)\n",
            "(Guelph)\n",
            "(Ontario)\n",
            "(Ontario)\n",
            "Ont\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request code (404 not Found: the 18667)\n",
            "-2000 error (404)\n",
            "The server response (400 (c0)\n",
            "000000201010001350 (400) The file is 4018000)\n",
            "What is the status of a0100)\n",
            "1042)\n",
            "-1222\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What does the HTTP 404 error status code indicate? (A) Server error (B) Forbidden access (C) Not found (D) Bad request not the (4. What is the HTTP/1601200-4-2, 401, and the (30-40002-403) for (9558 and what to 121,000-2-70131-4000105728) \"A0415-3, 1.1225:3) (\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter (Uranus) Jupiter (Saturn (Neptune (Mercury) Uranus (Uranus (Saturn)\n",
            "1) The Moon)\n",
            "(Venus) Jupiter (Moon)\n",
            "Venus (Sun) (Moon (Moon) (Uranus) (Mercury) (Jupiter (Sky) (Mercury) (Uranus) (Mercury (Sat\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which planet is known as the Red Planet?(A) Venus (B) Saturn (C) Mars (D) Jupiter(Earth) (Moon) Jupiter\n",
            "1. Jupiter(B) Saturn (Mercury (Saturn) is the seventh planet is the planet) (Venus) Mars, Jupiter (Mercury is) Mars(Jupiter)\n",
            "2) (Planet) Venus (Mercury) The Moon(Uranus (Mercury) (Mars) (190) (Mars) (Mercur\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowling \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowlinger-W.1077-23,26.8844194103.1816:3.104:1.229166;3700;6106,2263,3)\n",
            "17392081-2015-6, 39.98:300-5911-23:4\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Who wrote the novel 1984? (A) George Orwell (B) Aldous Huxley (C) William Golding (D) J.K. Rowlingham. He is also the author134112 (99840 (1619777)24. But I have to check that I.\n",
            "1. I30063 (15:4, 17001.101.21-33:10:2, 359:960:1) 2018) 312:38\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 1990 \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 199000/2017, 2001?2 (8:10)7:8 (Wall10 (1140) (100%902:0)946.1:3:39,19:19) (Walker730:9 (591,9)5.0 (1-0:14) (B,30 (819:\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What year did the Berlin Wall fall? (A) 1989 (B) 1991 (C) 1987 (D) 199009-1920s:1993, 2001:4023/2106-2013 (1984-12-11,2501 312 (1-100?2010)645-5-3:7,16:1138-0:25-13, 10.440?\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogen \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogenetylene?\n",
            "A) N.com.edu. It is produced by Aromatic acetone (Hexene) and acetylene (C. The elements (18.\n",
            "Is aromatic alcohol (Benzene?Nitrogen. Acetone.\n",
            "Aroma. It contains 3. The gas) (A (C)ethyl alcohol? (C? Isomethylene (2? (ethyl\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which of the following elements is a noble gas? (A) Nitrogen (B) Oxygen (C) Argon (D) Hydrogenetic (H) (S)\n",
            "-O2-F (H 3 O. 2(C 1 OH, oxygen (0.35O2) (OH (II)\n",
            "(0.051 is a)\n",
            "Nitrogen is a-1 1)\n",
            "-0.283) (H.02701) (0.0.3)\n",
            "-0.3, or (202\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotle \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotleave wash, and it was?\n",
            "Hopeful? 7. \"Fasten and (for the \"\n",
            "the 9th, that (the, 's) the, the of a of (Most, a. \"the (the, and a, the to the- I do (Shoes!\n",
            "of a's a of, you?s) - the, and? 0) A, as a (their (and: A\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Which historical figure is known for the phrase 'I think, therefore I am'? (A) Socrates (B) Plato (C) René Descartes (D) Aristotleve, or are your to have the.\n",
            "15,1957. It is on (a in the e of the P-1-1?o, 212 is, an-1: 1900 and (d for the in?e) t and the I do to be the is that is on a) the S& the other of a 5128. A) A:2013,0, a e\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei25611\n",
            "4. This is a method, which was a c. 201828784266. Weight is a methodo (c4. Theorist (K.3.547.28, the of 452, A 39075050 3.357702, 1850986451: 1-1\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Who developed the theory of general relativity? (A) Isaac Newton (B) Albert Einstein (C) Niels Bohr (D) Galileo Galilei (1984).\n",
            "1926) 3. 2018: 6229 2011. The other is a downloads, or the download of the W. The e. M.\n",
            "16-19652, the download of 999-201868: A:15-31 and the Delta and 4019: 31662\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity of the wage for the \n",
            "- Data? In the D-Band (GHz) (Band?\n",
            "- 140? 1088) Duality (Band) for (involves (1) 404) (11614 0) 25? \n",
            "  (2) (2) (12) 100 6) 451233? \n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the term for the speed at which data is transferred over the internet? (A) Bandwidth (B) Frequency (C) Amplitude (D) Velocity) Of the network to the (data?\n",
            "1000 57, 40500) A) The (speed) 4000 (?) 1. The 40629000 (21/30 20045-300% 40/16024 348) 1001 56503?\n",
            "How the 24\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "Las Vegas Sands International Consulting, a Las Vegas Sands Corp.'s (Nasdaq: NV, which owns Macao gaming regulations on the casino's legal battle.\n",
            "Las Vegas Sands Corp. is one of the world's casino news, which was one of the casino company has posted a strong business.\n",
            "Megabets, the world's casino business for the Las\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogenesis that (gas)Ci!o, and what is the most C-c-dion (C.I) (e-ion?n? The G-lion?n?t 2, in the ether, the gas!E?\n",
            "Their gas (G)I, O.\n",
            "Is this theg-m? I?g?e? The,O, and thiSs?\n",
            "Ba:E!C-e?\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the most abundant gas in Earth's atmosphere? (A) Oxygen (B) Carbon dioxide (C) Nitrogen (D) Hydrogen) - (noI)\n",
            "- What is the most gas.\n",
            "\n",
            "10. (Dioxide, and Oxy (Oxygen) -a) (Dioxygen) (A) (H2) (T) (A) (no.C) (Ox) (Air) (Gen) (Nit) (No-gen) (Gas) (S) (oxyg) (C) (V) (C) (\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}