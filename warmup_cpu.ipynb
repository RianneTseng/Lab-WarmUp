{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpjssr/Lab-WarmUp/blob/main/warmup_cpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypHsLdoUfDR2",
        "outputId": "8cdf7a43-ef28-4bd3-8c26-fb6e0ad400a8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=296593 sha256=96fb8fb888b0d859d964f1c7b2cfdba52fe8f5bea5bc27a20c9dafdaba802d6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import snapshot_download\n",
        "from llama_cpp import Llama\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ATpQwAnvywYV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder = \"/content/drive/MyDrive/lab/cpu\"\n",
        "llama_folder = root_folder + \"/llama.cpp\"\n",
        "hf_folder = root_folder + \"/hf\"\n",
        "output_folder = root_folder + \"/output\"\n",
        "\n",
        "main_path = llama_folder + \"/main\"\n",
        "quantize_path = llama_folder + \"/quantize\"\n",
        "\n",
        "hf_token=\"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\"\n",
        "gguf_model_name = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\"\n",
        "hf_model_name = \"TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_1_id = \"tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\"\n",
        "model_2_hf = root_folder + \"/hf\"\n",
        "model_2_id = \"TinyLlama/\" + hf_model_name\n",
        "model_2_path = root_folder + \"/\" + hf_model_name + \"-f16.gguf\"\n",
        "model_3_path = root_folder + \"/\" + hf_model_name + \"-Q8.gguf\"\n",
        "model_1_output_path = output_folder + \"/model_1.txt\"\n",
        "model_2_output_path = output_folder + \"/model_2.txt\"\n",
        "model_3_output_path = output_folder + \"/model_3.txt\"\n",
        "\n",
        "problem_set=[\n",
        "\"Explain the concept of gravity.\",\n",
        "\"Continue this conversation: So, do you think it will rain tomorrow?\",\n",
        "\"What is the derivative of x^2+3x+2?\",\n",
        "\"Discuss the significance of the Lunar New Year in East Asian cultures.\",\n",
        "\"Describe how a blockchain works.\",\n",
        "\"Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\",\n",
        "\"Write a short story about a detective solving a mystery in a futuristic city.\",\n",
        "\"Translate 'Hello, how are you?' into Japanese.\",\n",
        "\"Tell a joke about artificial intelligence.\",\n",
        "\"What are the implications of the latest financial news from Wall Street?\"\n",
        "]"
      ],
      "metadata": {
        "id": "rG2xje9iyx0e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd {llama_folder}\n",
        "!chmod 755 {main_path}\n",
        "!chmod 755 {quantize_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLGkzEvwyzlc",
        "outputId": "6a34b4e8-f51d-4f05-d116-76865611bfb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/lab/cpu/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJiOStTWg_0E"
      },
      "source": [
        "# Basic Challenge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1_id)\n",
        "print(root_folder)\n",
        "print(gguf_model_name)\n",
        "model_1_path = hf_hub_download(\n",
        "               filename=model_1_id,\n",
        "               local_dir=root_folder,\n",
        "               token=hf_token,\n",
        "               repo_id=gguf_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnoFeBvu6eNt",
        "outputId": "5073a061-f498-4fcb-bd4f-517f9c8c4f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\n",
            "/content/drive/MyDrive/lab/cpu\n",
            "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYnlATczGlCm",
        "outputId": "ac5c605d-6fcc-4152-d6b3-ec14c6148b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716476587\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/cpu/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_K:  135 tensors\n",
            "llama_model_loader: - type q6_K:   21 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 262/32003 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32003\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = py007_tinyllama-1.1b-chat-v0.3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32002 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size =   636.18 MiB\n",
            "......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    66.51 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s><|im_start|>user\n",
            "¿Cuál es la diferencia entre un lenguaje de programación que no tiene tipos y uno donde la programación se basa en operaciones con cadenas?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Claro, de la misma manera que un número no tiene tipo, sino que se basa en operaciones con cadenas, el tipo de datos para la programación basada en operaciones\n",
            "llama_print_timings:        load time =    1283.98 ms\n",
            "llama_print_timings:      sample time =       6.28 ms /   100 runs   (    0.06 ms per token, 15931.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   16867.67 ms /   100 runs   (  168.68 ms per token,     5.93 tokens per second)\n",
            "llama_print_timings:       total time =   17030.31 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ],
      "source": [
        "!./main -m {model_1_path} --n-predict 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12oLkQFggxTM"
      },
      "source": [
        "# Medium Challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "698fa1ae14b74f83a4ffafa4524ca60b",
            "081ce90f4fcc4e25b64f1d38a0a87b6d",
            "edc461bd86e740199ad96102ac4204b1",
            "062752c990994c9890b28d2c751f4ae8",
            "045d2ed73dbd449eb37ce210d9fb8a7b",
            "78587de87bd64ea7b243951912efcf43",
            "77f1516c9e174df88299247de79920f6",
            "7526b0c51a66465a814b688f828ca5ba",
            "7fad478b39f74b628f0b09ad69404b48",
            "c1de7d9fafde4a14a7b8e42296df0725",
            "47f7bc1f422b411d8b4fff887a2d60c1"
          ]
        },
        "id": "YhmjbZyJt4VE",
        "outputId": "c9a1f64e-e764-4dd8-f218-e962a34c840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "698fa1ae14b74f83a4ffafa4524ca60b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:convert:Loading model file /content/drive/MyDrive/lab/cpu/hf/model.safetensors\n",
            "INFO:convert:model parameters count : 1100048384 (1B)\n",
            "INFO:convert:params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('/content/drive/MyDrive/lab/cpu/hf'))\n",
            "INFO:convert:Loaded vocab file PosixPath('/content/drive/MyDrive/lab/cpu/hf/tokenizer.model'), type 'spm'\n",
            "INFO:convert:Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "INFO:convert:Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens unset>\n",
            "INFO:convert:Writing /content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf, format 1\n",
            "WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 2\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "INFO:convert:[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type F16  | T+   3\n",
            "INFO:convert:[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type F16  | T+   4\n",
            "INFO:convert:[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   4\n",
            "INFO:convert:[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   4\n",
            "INFO:convert:[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   5\n",
            "INFO:convert:[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "INFO:convert:[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   6\n",
            "INFO:convert:[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   6\n",
            "INFO:convert:[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   6\n",
            "INFO:convert:[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   7\n",
            "INFO:convert:[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   7\n",
            "INFO:convert:[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   7\n",
            "INFO:convert:[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   7\n",
            "INFO:convert:[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   8\n",
            "INFO:convert:[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   8\n",
            "INFO:convert:[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   8\n",
            "INFO:convert:[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   8\n",
            "INFO:convert:[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   9\n",
            "INFO:convert:[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   9\n",
            "INFO:convert:[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   9\n",
            "INFO:convert:[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   9\n",
            "INFO:convert:[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "INFO:convert:[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "INFO:convert:[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "INFO:convert:[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "INFO:convert:[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "INFO:convert:[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  11\n",
            "INFO:convert:[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  11\n",
            "INFO:convert:[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "INFO:convert:[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  12\n",
            "INFO:convert:[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  12\n",
            "INFO:convert:[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  13\n",
            "INFO:convert:[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  13\n",
            "INFO:convert:[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  13\n",
            "INFO:convert:[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "INFO:convert:[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  14\n",
            "INFO:convert:[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  14\n",
            "INFO:convert:[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "INFO:convert:[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "INFO:convert:[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "INFO:convert:[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  16\n",
            "INFO:convert:[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  16\n",
            "INFO:convert:[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  16\n",
            "INFO:convert:[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  16\n",
            "INFO:convert:[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+  17\n",
            "INFO:convert:[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  17\n",
            "INFO:convert:[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  18\n",
            "INFO:convert:[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  18\n",
            "INFO:convert:[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  18\n",
            "INFO:convert:[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  19\n",
            "INFO:convert:[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  19\n",
            "INFO:convert:[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "INFO:convert:[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "INFO:convert:[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  20\n",
            "INFO:convert:[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  21\n",
            "INFO:convert:[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "INFO:convert:[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  22\n",
            "INFO:convert:[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  22\n",
            "INFO:convert:[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  22\n",
            "INFO:convert:[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  23\n",
            "INFO:convert:[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  23\n",
            "INFO:convert:[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  23\n",
            "INFO:convert:[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+  23\n",
            "INFO:convert:[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  24\n",
            "INFO:convert:[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "INFO:convert:[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "INFO:convert:[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  25\n",
            "INFO:convert:[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  25\n",
            "INFO:convert:[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  25\n",
            "INFO:convert:[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  25\n",
            "INFO:convert:[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  26\n",
            "INFO:convert:[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  26\n",
            "INFO:convert:[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  26\n",
            "INFO:convert:[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+  26\n",
            "INFO:convert:[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  26\n",
            "INFO:convert:[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  27\n",
            "INFO:convert:[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  27\n",
            "INFO:convert:[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+  27\n",
            "INFO:convert:[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  27\n",
            "INFO:convert:Wrote /content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf\n"
          ]
        }
      ],
      "source": [
        "snapshot_download(\n",
        "    repo_id=model_2_id,\n",
        "    local_dir=model_2_hf,\n",
        "    local_dir_use_symlinks=False,\n",
        "    revision=\"main\",\n",
        "    use_auth_token=hf_token)\n",
        "!python convert.py {hf_folder} --outfile {model_2_path} --outtype f16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s2lJmVxuY3x",
        "outputId": "0dfd8b41-8e04-405a-944c-1f91bca5c743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716476636\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 2.05 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2098.35 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    66.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "1. Open a new tab in your web browser and go to https://www.google.com.\n",
            "2. In the address bar, type the phrase \"Google search\" (without quotation marks).\n",
            "3. Click on the magnifying glass icon (the letter \"M\").\n",
            "4. Type in your search terms, and hit enter.\n",
            "5. Google will return search results, sorted by relevance.\n",
            "6. Click on a result to\n",
            "llama_print_timings:        load time =    8844.98 ms\n",
            "llama_print_timings:      sample time =       8.80 ms /   100 runs   (    0.09 ms per token, 11358.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   30621.16 ms /   100 runs   (  306.21 ms per token,     3.27 tokens per second)\n",
            "llama_print_timings:       total time =   30744.60 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ],
      "source": [
        "!./main -m {model_2_path} --n-predict 100 --ignore-eos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWbca2ui-b2a"
      },
      "source": [
        "# Advanced Challenge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh3NZkdA-mAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d973d0-9852-443c-e391-0b7ea033e512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf' to '/content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf' as Q8_0\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type  f16:  156 tensors\n",
            "[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q8_0 .. size =   125.00 MiB ->    66.41 MiB\n",
            "[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q8_0 .. size =    22.00 MiB ->    11.69 MiB\n",
            "[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q8_0 .. size =     1.00 MiB ->     0.53 MiB\n",
            "[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "llama_model_quantize_internal: model size  =  2098.35 MB\n",
            "llama_model_quantize_internal: quant size  =  1114.91 MB\n",
            "\n",
            "main: quantize time = 18374.14 ms\n",
            "main:    total time = 18374.14 ms\n"
          ]
        }
      ],
      "source": [
        "!./quantize {model_2_path} {model_3_path} Q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPKp9oE__SjU",
        "outputId": "ebd5831f-bd36-4043-c797-997534d2b850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2961 (201cc11a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1716476697\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /content/drive/MyDrive/lab/cpu/TinyLlama-1.1B-Chat-v1.0-Q8.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = hf\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q8_0:  156 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 1.09 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = hf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size =  1114.91 MiB\n",
            "..........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    66.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 100, n_keep = 1\n",
            "\n",
            "\n",
            "<s> <|system|>\n",
            "\n",
            "1. Can you paraphrase the section on the importance of maintaining a work-life balance for professionals in the tech industry, and provide examples of how it can impact their job satisfaction, productivity, and career progression?\n",
            "\n",
            "2. Can you explain how setting boundaries and prioritizing self-care can help professionals manage stress and increase their productivity during work hours? Please provide specific examples of activities or practices that are effective\n",
            "llama_print_timings:        load time =    1499.33 ms\n",
            "llama_print_timings:      sample time =       7.79 ms /   100 runs   (    0.08 ms per token, 12835.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   22081.24 ms /   100 runs   (  220.81 ms per token,     4.53 tokens per second)\n",
            "llama_print_timings:       total time =   22179.85 ms /   100 tokens\n",
            "Log end\n"
          ]
        }
      ],
      "source": [
        "!./main -m {model_3_path} --n-predict 100 --ignore-eos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr7WHkFWBene"
      },
      "source": [
        "# Bonus Challenge\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i_range=tqdm(range(10))\n",
        "for i in i_range:\n",
        "  problem = \"\\\"\" + problem_set[i] + \"\\\"\"\n",
        "\n",
        "  #model 1 (q4)\n",
        "  !./main -m {model_1_path} > {model_1_output_path} --n-predict 100 -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 2 (f16)\n",
        "  !./main -m {model_2_path} > {model_2_output_path} --n-predict 100 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  #model 3 (q8)\n",
        "  !./main -m {model_3_path} > {model_3_output_path} --n-predict 100 --ignore-eos -p {problem}\n",
        "  clear_output()\n",
        "  i_range.refresh()\n",
        "\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  open(path, 'w').close()\n",
        "  f = open(path, \"a\")\n",
        "  f.write(\"model 1 (q4)\\n\")\n",
        "  model_file = open(model_1_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 2 (f16)\\n\")\n",
        "  model_file = open(model_2_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.write(\"\\n\\nmodel 3 (q8)\\n\")\n",
        "  model_file = open(model_3_output_path, \"r\")\n",
        "  f.write(model_file.read())\n",
        "  model_file.close()\n",
        "  f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l14pPryl2IkX",
        "outputId": "8ff4ace6-8c80-4f12-e79e-6f17db506169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [10:46<00:00, 64.67s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  path = output_folder + \"/\" + str(i) + \".txt\"\n",
        "  f = open(path, \"r\")\n",
        "  print(\"Problem: \",problem_set[i],\"\\n\")\n",
        "  print(f.read())\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAEG4DtNgxof",
        "outputId": "18118e2a-04fc-4eb1-d274-4e7666c5e451"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem:  Explain the concept of gravity. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Explain the concept of gravity.\n",
            "Explain the concept of the Earth's gravitational field.\n",
            "Describe the concept of an object at rest staying at rest, and an object in motion continuously moving towards an object at rest.\n",
            "Describe the concept of an object in motion moving at a constant speed in a straight line.\n",
            "Describe the concept of an object at rest remaining at rest, and an object moving continuously moving towards an object at rest.\n",
            "Describe the concept of a force that acts\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Explain the concept of gravity. What is the force of gravity, and how does it affect the motion of objects? How does gravity affect the speed and direction of objects in free space? How does gravity affect the motion of objects in gravitational fields? Use examples to illustrate your explanation. Provide a clear and concise explanation with appropriate technical terms and diagrams when appropriate. Include a summary at the end that summarizes your main points. Your response should be between 500 and 700 words, and\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Explain the concept of gravity. How does gravity affect our daily lives, and what are some everyday examples of its effects?\n",
            "\n",
            "1. Gravity affects the Earth's rotation: Gravity works in the opposite direction to the Earth's rotation. The Earth rotates around its axis every day, but it takes about 24 hours for it to complete a full rotation. Gravity causes the Earth's rotation to slow down, so it takes longer to complete each day.\n",
            "\n",
            "2. Gravitymodel 1 (q4)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Continue this conversation: So, do you think it will rain tomorrow? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow?\n",
            "Can you tell us more about your experience in the area?\n",
            "Do you have a favorite place to eat in the area?\n",
            "Do you have any recommendations for things to do in the area?\n",
            "How about a special request?\n",
            "Do you want to know about the weather or the local scene in general?\n",
            "Can you tell us about something you know is awesome?\n",
            "What is the best activity to do in the area?\n",
            "Can you tell us more about the people and culture in\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow?\n",
            "Yes, it's supposed to rain.\n",
            "No, it won't rain tomorrow.\n",
            "I hope it will rain tomorrow.\n",
            "I am going to the beach tomorrow.\n",
            "I don't know if it will rain tomorrow.\n",
            "I've been to the beach tomorrow.\n",
            "I'm not sure if it will rain tomorrow.\n",
            "I'll see if it rains tomorrow.\n",
            "It's raining now. It was ra\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Continue this conversation: So, do you think it will rain tomorrow? Do you have a favorite place to go for a picnic? Are you excited about the new school year? Do you have any plans for the weekend? Could you paraphrase the text material in a simpler language? Do you think it will rain tomorrow? I am going to go for a picnic. I love the new school year! I am excited for the weekend. I have no plans for the weekend. I will be seeing my friends. How do you think\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What is the derivative of x^2+3x+2? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What is the derivative of x^2+3x+2?\n",
            "In maths, the derivative of x^2+3x+2 is 2x^2 + 4x + 1.\n",
            "The derivative of 2x^2+3x+2 is 2x^2 + 4x + 1.\n",
            "The derivative of 2x^2+3x+2 at x = 2 is 2x^2 + 4x + 1.\n",
            "In maths, the derivative of \n",
            "\n",
            "model 2 (f16)\n",
            "<s> What is the derivative of x^2+3x+2?\n",
            "\n",
            "The derivative of a function is the change in the function with respect to the variable.\n",
            "\n",
            "a. In the case of x^2+3x+2\n",
            "\n",
            "The derivative of x^2+3x+2 at x=0 is\n",
            "\n",
            "(x+3)dx = 3x^2 + 3x + 2\n",
            "\n",
            "so the derivative is\n",
            "\n",
            "f'(0) = 6\n",
            "\n",
            "b. In the case of sin(x\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What is the derivative of x^2+3x+2?\n",
            "> Given: x^2+3x+2\n",
            "> Derivative: x(x^2+3x+2)\n",
            ">\n",
            "> Check:\n",
            "> - The derivative of x^2+3x+2 is x(x^2+3x+2)\n",
            "> - The derivative of x(x^2+3x+2) is 2x(x^2+3x+2)\n",
            "> - The derivative of 2\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Discuss the significance of the Lunar New Year in East Asian cultures. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures.\n",
            "Discuss the significance of the Lunar New Year in China's contemporary cultural and economic rise.\n",
            "Discuss the role of the Chinese New Year as a springboard for Chinese cultural imperialism, economic rise, and foreign policy maneuvers.\n",
            "Discuss the influence of the Lunar New Year on the traditional Chinese calendar.\n",
            "Discuss the significance of the Lunar New Year for the Chinese calendar and Chinese tradition.\n",
            "Discuss the importance of the Chinese New Year\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures. Explain how it has evolved over time and how it has influenced the way people celebrate the holiday. Additionally, discuss the ways in which the Lunar New Year is celebrated in different regions of East Asia, and the unique traditions and customs associated with this holiday. Finally, discuss the role of the Lunar New Year in East Asian cultural identity and its importance in shaping the identity of the East Asian diaspora. Your essay should be well-\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Discuss the significance of the Lunar New Year in East Asian cultures. Explain how it has evolved over time, and how it is celebrated today. Provide examples of traditional practices, such as fireworks displays, food offerings, and traditional clothing, to illustrate its cultural significance. Finally, discuss how the Lunar New Year is celebrated in different regions of East Asia and how it differs from other holidays in the same region. Use credible sources to support your analysis. Use clear and concise language with a professional writing style. Be\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Describe how a blockchain works. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Describe how a blockchain works.\n",
            "Hypothesize how blockchains can be used to create decentralized applications.\n",
            "Describe the security implications of the use of blockchains for decentralized applications.\n",
            "Explain the benefits and challenges of using blockchains for decentralized applications.\n",
            "Describe the role of smart contracts in blockchain technology and their potential use in creating decentralized applications.\n",
            "Describe the structure and functionality of a blockchain-based decentralized application\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Describe how a blockchain works. Include the following:\n",
            "\n",
            "- Definition of blockchain\n",
            "- How blocks are added to the chain\n",
            "- Transactions are added to the blockchain\n",
            "- Encryption and signing of transactions\n",
            "- How the blockchain validates transactions\n",
            "- How blocks are verified and added to the blockchain\n",
            "\n",
            "Describe the potential uses and benefits of a blockchain for different industries. Include specific examples of how blockchain can improve security, transparency, efficiency, and trust in transactions\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Describe how a blockchain works. Explain how it works as a decentralized ledger and provide an example. Discuss the potential benefits and limitations of blockchain technology as a way of storing and sharing sensitive data. Provide specific examples of how blockchain is being used to create secure and transparent transactions, including virtual currencies and financial institutions. Provide recommendations for how businesses can benefit from using blockchain technology, including the potential for enhanced security and transparency. Provide examples of how blockchain can\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not? What are the challenges and limitations of prioritizing pedestrians over self-driving cars? Why do some people still believe in the value of people as guides for a self-driving car?<|im_end|>\n",
            "Assistant\n",
            "\n",
            "Prioritizing the safety of pedestrians over people is not a decision that should be taken lightly. There are many potential consequences of not prioritizing pedestrians, including:\n",
            "\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\n",
            "\n",
            "4. Do you think self-driving cars will ever replace human drivers? Why or why not?\n",
            "\n",
            "5. In your opinion, what are the benefits of self-driving cars for the transportation industry as a whole?\n",
            "\n",
            "6. What is your vision for the future of transportation and how do you think self-driving cars will contribute to it?\n",
            "\n",
            "7. What are some potential challenges or obstacles that self-driv\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Should self-driving cars prioritize the safety of passengers over pedestrians? Why or why not?\n",
            "\n",
            "Answer: Self-driving cars prioritize the safety of passengers over pedestrians, as they have the technology to monitor their surroundings and anticipate potential hazards. While self-driving cars cannot physically see pedestrians or other cars, they can use advanced sensors and cameras to detect potential hazards and take appropriate safety measures, such as slowing down or stopping if necessary. This approach ensures that the safety of\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Write a short story about a detective solving a mystery in a futuristic city. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city. The story should take place in an urban setting with plenty of crime and corruption. The story should also be written in first-person. In the story, the detective should use his or her skills of deduction, observation, and problem-solving to unravel the mystery and bring the perpetrator to justice. The story should be well-written, intriguing, and engaging, with a good plot, believable characters, and a satisfying resolution. The story\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city. Use vivid descriptions to create a sense of the futuristic setting and describe the detective's interactions with the various characters in the story. Incorporate technology and advancements in technology to create a futuristic feel for the story. Make the mystery complex and unexpected, with unexpected twists and turns that keep the reader engaged. Use a third-person perspective, and include dialogue between characters to deepen the story's depth. Consider using descriptive language and\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Write a short story about a detective solving a mystery in a futuristic city. The story should be set in the year 2050 and feature advanced technology, such as robots or advanced sensors, and should have a fast-paced, action-packed tone. The detective should use their intelligence, intuition, and technical skills to solve the mystery, and the setting should be described in vivid detail to immerse the reader in the world of the story. Additionally, the story should have a clear resolution that ties up all loose ends\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Translate 'Hello, how are you?' into Japanese. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Translate 'Hello, how are you?' into Japanese.\n",
            "The problem with this sentence is that it doesn't make much sense in English. The word \"Hello\" is not in the sentence, nor is the word \"how\". To make the sentence make sense, you'd need to add some context or information to make sense of the sentence.\n",
            "Is this sentence grammatically correct?\n",
            "The grammatical form of the sentence is \"present simple\" (which means it has some kind of \"present\" in the middle, but that'\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Translate 'Hello, how are you?' into Japanese. Write the translation in Japanese and English. Example: 私は何をしますか。こんにちは。 \n",
            "\n",
            "3. Write a step-by-step guide on how to create a successful business pitch. Write the guide in a clear and concise manner, including tips on what to include in the pitch, how to structure it, and how to address potential objections. Make sure to provide examples of successful pitches and break down the process into smaller\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Translate 'Hello, how are you?' into Japanese. Use a formal tone and a polite manner. 3. \"Ichimai desu, hachigatsu no Natsu\" (Good day, summer season) - Translate 'Good day, summer season' into Japanese. Use a formal tone and a friendly manner. 4. \"Nihon ni wa hikōshi desu ka?\" (What is Japan like?) - Translate 'What is Japan like?' into Japanese. Use a casual\n",
            "\n",
            "\n",
            "\n",
            "Problem:  Tell a joke about artificial intelligence. \n",
            "\n",
            "model 1 (q4)\n",
            "<s> Tell a joke about artificial intelligence.\n",
            "This will help you understand what I mean when I say that you should not trust the internet.\n",
            "Okay, so let's say you invented a super-intelligent AI that can learn and make decisions just like humans do.\n",
            "But it's so smart that it's able to out-think even the best doctors at the latest medical breakthroughs.\n",
            "It even knows when to stop surgery.\n",
            "But, you say, who would\n",
            "\n",
            "model 2 (f16)\n",
            "<s> Tell a joke about artificial intelligence. AI is being used in all forms of entertainment, from movies to TV shows, and even in music videos. AI-powered comedy sketches are no exception. The following three examples use AI to create funny sketches:\n",
            "\n",
            "1. \"Wonder Woman 1984\" - AI-powered humor duo, \"Trolls AI,\" created a hilarious sketch for the movie \"Wonder Woman 198\n",
            "\n",
            "model 3 (q8)\n",
            "<s> Tell a joke about artificial intelligence. The joke goes like this:\n",
            "\n",
            "1. A scientist was walking home from the lab one night. As he was crossing the street, he noticed a car speeding up to overtake him.\n",
            "\n",
            "2. The scientist quickly turned around and shouted, \"Watch it, man!\"\n",
            "\n",
            "3. The car, a little confused, slowed down and drove around the scientist, who was now stuck in a ditch.\n",
            "\n",
            "4. The car stopped,\n",
            "\n",
            "\n",
            "\n",
            "Problem:  What are the implications of the latest financial news from Wall Street? \n",
            "\n",
            "model 1 (q4)\n",
            "<s> What are the implications of the latest financial news from Wall Street?\n",
            "What are the implications of the latest financial news from Wall Street?\n",
            "Bond investors took the biggest hit. Stocks took the second biggest hit.\n",
            "What were the biggest takeaways from the latest financial news from Wall Street?\n",
            "Here are the most important things to know:\n",
            "The Federal Reserve raised rates three times this year. It's now the Fed's job to keep them there.\n",
            "The central banks of the world are expected to raise rates more often.\n",
            "\n",
            "model 2 (f16)\n",
            "<s> What are the implications of the latest financial news from Wall Street? Why did the market plunge on Friday? What could be the next potential catalysts for the market? Answer according to:\n",
            "The latest news from Wall Street was mixed on Friday, as the US stock market plunged after a report showed that the US economy grew at a slower-than-expected pace last quarter.\n",
            "The Dow Jones Industrial Average fell 1.1% to 15,306.62, while the S&P 50\n",
            "\n",
            "model 3 (q8)\n",
            "<s> What are the implications of the latest financial news from Wall Street? Answer according to: The Dow Jones Industrial Average was up 41.16 points or 0.31% at 24,604.95. The Standard & Poor's 500 Index was up 4.86 points or 0.20% at 2,663.77. The Nasdaq Composite Index was up 19.40 points or 0.37%\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOleODnbLex0gk3y7ht0OPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "698fa1ae14b74f83a4ffafa4524ca60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_081ce90f4fcc4e25b64f1d38a0a87b6d",
              "IPY_MODEL_edc461bd86e740199ad96102ac4204b1",
              "IPY_MODEL_062752c990994c9890b28d2c751f4ae8"
            ],
            "layout": "IPY_MODEL_045d2ed73dbd449eb37ce210d9fb8a7b"
          }
        },
        "081ce90f4fcc4e25b64f1d38a0a87b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78587de87bd64ea7b243951912efcf43",
            "placeholder": "​",
            "style": "IPY_MODEL_77f1516c9e174df88299247de79920f6",
            "value": "Fetching 10 files: 100%"
          }
        },
        "edc461bd86e740199ad96102ac4204b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7526b0c51a66465a814b688f828ca5ba",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fad478b39f74b628f0b09ad69404b48",
            "value": 10
          }
        },
        "062752c990994c9890b28d2c751f4ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1de7d9fafde4a14a7b8e42296df0725",
            "placeholder": "​",
            "style": "IPY_MODEL_47f7bc1f422b411d8b4fff887a2d60c1",
            "value": " 10/10 [00:00&lt;00:00,  7.92it/s]"
          }
        },
        "045d2ed73dbd449eb37ce210d9fb8a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78587de87bd64ea7b243951912efcf43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f1516c9e174df88299247de79920f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7526b0c51a66465a814b688f828ca5ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fad478b39f74b628f0b09ad69404b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1de7d9fafde4a14a7b8e42296df0725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47f7bc1f422b411d8b4fff887a2d60c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}